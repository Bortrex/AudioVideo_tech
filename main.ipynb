{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook settings"
      ],
      "metadata": {
        "id": "Ow_LU16JQ9j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq install neofetch\n",
        "!pip --quiet install wandb -qU\n",
        "!pip --quiet install profilegraph"
      ],
      "metadata": {
        "id": "ZMM5owPmQ-ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "!neofetch"
      ],
      "metadata": {
        "id": "7NQyQRDOSuMJ",
        "outputId": "eb53c14e-04e3-4d91-cb11-15d360eb76f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb 19 11:27:28 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\u001b[?25l\u001b[?7l\u001b[0m\u001b[31m\u001b[1m            .-/+oossssoo+/-.\n",
            "        `:+ssssssssssssssssss+:`\n",
            "      -+ssssssssssssssssssyyssss+-\n",
            "    .ossssssssssssssssss\u001b[37m\u001b[0m\u001b[1mdMMMNy\u001b[0m\u001b[31m\u001b[1msssso.\n",
            "   /sssssssssss\u001b[37m\u001b[0m\u001b[1mhdmmNNmmyNMMMMh\u001b[0m\u001b[31m\u001b[1mssssss/\n",
            "  +sssssssss\u001b[37m\u001b[0m\u001b[1mhm\u001b[0m\u001b[31m\u001b[1myd\u001b[37m\u001b[0m\u001b[1mMMMMMMMNddddy\u001b[0m\u001b[31m\u001b[1mssssssss+\n",
            " /ssssssss\u001b[37m\u001b[0m\u001b[1mhNMMM\u001b[0m\u001b[31m\u001b[1myh\u001b[37m\u001b[0m\u001b[1mhyyyyhmNMMMNh\u001b[0m\u001b[31m\u001b[1mssssssss/\n",
            ".ssssssss\u001b[37m\u001b[0m\u001b[1mdMMMNh\u001b[0m\u001b[31m\u001b[1mssssssssss\u001b[37m\u001b[0m\u001b[1mhNMMMd\u001b[0m\u001b[31m\u001b[1mssssssss.\n",
            "+ssss\u001b[37m\u001b[0m\u001b[1mhhhyNMMNy\u001b[0m\u001b[31m\u001b[1mssssssssssss\u001b[37m\u001b[0m\u001b[1myNMMMy\u001b[0m\u001b[31m\u001b[1msssssss+\n",
            "oss\u001b[37m\u001b[0m\u001b[1myNMMMNyMMh\u001b[0m\u001b[31m\u001b[1mssssssssssssss\u001b[37m\u001b[0m\u001b[1mhmmmh\u001b[0m\u001b[31m\u001b[1mssssssso\n",
            "oss\u001b[37m\u001b[0m\u001b[1myNMMMNyMMh\u001b[0m\u001b[31m\u001b[1msssssssssssssshmmmh\u001b[0m\u001b[31m\u001b[1mssssssso\n",
            "+ssss\u001b[37m\u001b[0m\u001b[1mhhhyNMMNy\u001b[0m\u001b[31m\u001b[1mssssssssssss\u001b[37m\u001b[0m\u001b[1myNMMMy\u001b[0m\u001b[31m\u001b[1msssssss+\n",
            ".ssssssss\u001b[37m\u001b[0m\u001b[1mdMMMNh\u001b[0m\u001b[31m\u001b[1mssssssssss\u001b[37m\u001b[0m\u001b[1mhNMMMd\u001b[0m\u001b[31m\u001b[1mssssssss.\n",
            " /ssssssss\u001b[37m\u001b[0m\u001b[1mhNMMM\u001b[0m\u001b[31m\u001b[1myh\u001b[37m\u001b[0m\u001b[1mhyyyyhdNMMMNh\u001b[0m\u001b[31m\u001b[1mssssssss/\n",
            "  +sssssssss\u001b[37m\u001b[0m\u001b[1mdm\u001b[0m\u001b[31m\u001b[1myd\u001b[37m\u001b[0m\u001b[1mMMMMMMMMddddy\u001b[0m\u001b[31m\u001b[1mssssssss+\n",
            "   /sssssssssss\u001b[37m\u001b[0m\u001b[1mhdmNNNNmyNMMMMh\u001b[0m\u001b[31m\u001b[1mssssss/\n",
            "    .ossssssssssssssssss\u001b[37m\u001b[0m\u001b[1mdMMMNy\u001b[0m\u001b[31m\u001b[1msssso.\n",
            "      -+sssssssssssssssss\u001b[37m\u001b[0m\u001b[1myyy\u001b[0m\u001b[31m\u001b[1mssss+-\n",
            "        `:+ssssssssssssssssss+:`\n",
            "            .-/+oossssoo+/-.\u001b[0m\n",
            "\u001b[20A\u001b[9999999D\u001b[43C\u001b[0m\u001b[1m\u001b[31m\u001b[1mroot\u001b[0m@\u001b[31m\u001b[1m17c2c6374ec8\u001b[0m \n",
            "\u001b[43C\u001b[0m-----------------\u001b[0m \n",
            "\u001b[43C\u001b[0m\u001b[31m\u001b[1mOS\u001b[0m\u001b[0m:\u001b[0m Ubuntu 20.04.5 LTS x86_64\u001b[0m \n",
            "\u001b[43C\u001b[0m\u001b[31m\u001b[1mHost\u001b[0m\u001b[0m:\u001b[0m Google Compute Engine\u001b[0m \n",
            "\u001b[43C\u001b[0m\u001b[31m\u001b[1mKernel\u001b[0m\u001b[0m:\u001b[0m 5.10.147+\u001b[0m \n",
            "\u001b[43C\u001b[0m\u001b[31m\u001b[1mUptime\u001b[0m\u001b[0m:\u001b[0m 9 mins\u001b[0m \n",
            "\u001b[43C\u001b[0m\u001b[31m\u001b[1mPackages\u001b[0m\u001b[0m:\u001b[0m 1406 (dpkg)\u001b[0m \n",
            "\u001b[43C\u001b[0m\u001b[31m\u001b[1mShell\u001b[0m\u001b[0m:\u001b[0m bash 5.0.17\u001b[0m \n",
            "\u001b[43C\u001b[0m\u001b[31m\u001b[1mTerminal\u001b[0m\u001b[0m:\u001b[0m jupyter-noteboo\u001b[0m \n",
            "\u001b[43C\u001b[0m\u001b[31m\u001b[1mCPU\u001b[0m\u001b[0m:\u001b[0m Intel Xeon (2) @ 2.299GHz\u001b[0m \n",
            "\u001b[43C\u001b[0m\u001b[31m\u001b[1mMemory\u001b[0m\u001b[0m:\u001b[0m 1174MiB / 12985MiB\u001b[0m \n",
            "\n",
            "\u001b[43C\u001b[30m\u001b[40m   \u001b[31m\u001b[41m   \u001b[32m\u001b[42m   \u001b[33m\u001b[43m   \u001b[34m\u001b[44m   \u001b[35m\u001b[45m   \u001b[36m\u001b[46m   \u001b[37m\u001b[47m   \u001b[m\n",
            "\u001b[43C\u001b[38;5;8m\u001b[48;5;8m   \u001b[38;5;9m\u001b[48;5;9m   \u001b[38;5;10m\u001b[48;5;10m   \u001b[38;5;11m\u001b[48;5;11m   \u001b[38;5;12m\u001b[48;5;12m   \u001b[38;5;13m\u001b[48;5;13m   \u001b[38;5;14m\u001b[48;5;14m   \u001b[38;5;15m\u001b[48;5;15m   \u001b[m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[?25h\u001b[?7h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "ylsitXJrbVH4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timing: 0h:2m:42s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([412, 311, 741, 337, 396, 674, 692, 152, 918, 788, 907, 270, 776,\n",
              "       486, 908, 494, 754, 854, 392, 105, 958, 317, 691, 554,  76])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# https://dvelopery0115.github.io/2021/08/01/Introduction_to_W&B.html\n",
        "# https://www.tensorflow.org/guide/tf_numpy # numpy report\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pprint\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "# loading profiler to check usage\n",
        "%load_ext profilegraph\n",
        "\n",
        "def asHHMMSS(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    h = math.floor(m / 60)\n",
        "    m -= h * 60\n",
        "    return '%dh:%dm:%ds'% (h, m, s)\n",
        "\n",
        "\n",
        "print(f\"Timing: {asHHMMSS(162)}\")\n",
        "N_agents = 2\n",
        "\n",
        "# classic_online - simple\n",
        "# setting seed\n",
        "SEED= 1111\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Log in to your W&B account\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = 'a956746b2267924a17a7ec60afa5aca91090843d'\n",
        "# os.environ[\"WANDB_MODE\"] = \"online\"\n",
        "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"DQN model\"\n",
        "\n",
        "\n",
        "SEEDS = np.random.randint(1000, size=25)\n",
        "SEEDS # 788 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fstZsNU_Q0Hz",
        "outputId": "0c735b7c-60fd-4d80-870a-8476e9dd68b2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_mbzHv6Q0H2"
      },
      "outputs": [],
      "source": [
        "from utils import foo\n",
        "from models import DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action space size: 15\n"
          ]
        }
      ],
      "source": [
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "def make_plots(step_values, ylabel=\"Rewards\", title='Player 1',\n",
        "               p_last_mean=False):\n",
        "    plt.figure(2, figsize=(14, 9))\n",
        "    plt.clf()\n",
        "    # durations_t = torch.tensor(step_values, dtype=torch.float)\n",
        "    durations_t = np.array(step_values, dtype=float)\n",
        "    \n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.grid(True, alpha=.45)\n",
        "    # plt.plot(durations_t.numpy())\n",
        "    plt.plot(durations_t, alpha=.6)\n",
        "    \n",
        "    # # Take 100 step averages and plot them too\n",
        "    n_EPI = 100\n",
        "    if len(durations_t) >= n_EPI:\n",
        "        means = [np.mean(durations_t[i-n_EPI:i]) for i in range(n_EPI, len(durations_t) + 1)]\n",
        "        # means = np.concatenate((np.zeros(99), means))\n",
        "        plt.plot(means)\n",
        "        # means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        # means = torch.cat((torch.zeros(99), means))\n",
        "        # plt.plot(means.numpy())\n",
        "    \n",
        "    plt.title(f'{title} training... stabilizes at {means[-1]:.4f}') \\\n",
        "              if p_last_mean else plt.title(f'{title} training...') \n",
        "\n",
        "    ax = plt.gca()\n",
        "    ax.xaxis.set_major_formatter(matplotlib.ticker.StrMethodFormatter('{x:,.0f}'))\n",
        "    # plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    # if is_ipython:\n",
        "    #     display.clear_output(wait=True)\n",
        "    #     display.display(plt.gcf())\n",
        "\n",
        "###### Functions for the Agents\n",
        "\n",
        "def replay_classic_reward(action):\n",
        "    # Compute profits for all agents\n",
        "    price = actions_space[action]\n",
        "    demand = np.exp((QUALITY - price) / HORIZON)\n",
        "    demand = demand / (np.sum(demand) + 1.) #np.exp(a0 / HORIZON))\n",
        "    reward = np.multiply(price - MARGIN_COST, demand)\n",
        "    return reward\n",
        "\n",
        "\n",
        "actions_space = np.arange(1.43, 2.0, 0.04) # 15 actions\n",
        "QUALITY = np.full((N_agents), 2) #np.ones(2) * 2\n",
        "MARGIN_COST = np.ones(N_agents)\n",
        "HORIZON = 1 / 4\n",
        "a0 = 0  # initial action\n",
        "N_actions = actions_space.size\n",
        "N_agents = 2\n",
        "\n",
        "reward_sum = np.array([5.58232796, 5.78802889, 5.92606135, 5.99644584, 6.00067233,\n",
        "                       5.94172477, 5.82402394, 5.65328833, 5.43631956, 5.18072579,\n",
        "                       4.89460298, 4.58619785, 4.26357789, 3.93433261, 3.60532586])\n",
        "\n",
        "\n",
        "print(f'Action space size: {N_actions}') # 15"
      ],
      "metadata": {
        "id": "qV-Y-suQQ0H3",
        "outputId": "d43b4657-8c89-4d1b-ecb7-09b68a481857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ],
      "metadata": {
        "id": "_41Z4yScQ0H3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN "
      ],
      "metadata": {
        "id": "y-Vc-zJIcBf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd \n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from itertools import count\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "\n",
        "DRATE = 0.95\n",
        "MAX_NORM = 1.0\n",
        "# TARGET_UPDATE = 10000\n",
        "\n",
        "# BATCH_SIZE = 256\n",
        "BATCH_SIZE = 128\n",
        "# BATCH_SIZE = 64\n",
        "# LR = 1e-4\n",
        "# LR = 4e-4 # 0.0004\n",
        "\n",
        "\n",
        "# USE_CUDA = False\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
        "\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(device)\n",
        "torch.cuda.empty_cache() # cleaning CUDA\n",
        "\n",
        "class DQN_agent(nn.Module):\n",
        "    def __init__(self, N_space, N_actions, tuned_q, agent_name=\"DQN\"):\n",
        "        super(DQN_agent, self).__init__()\n",
        "\n",
        "        self.epsilon = 1.  # initial exploration prob\n",
        "        self.ep_end = 0.01 # modified given it requires less visited -- only 1%\n",
        "        self.decay_rate = .00001   # 1e-5\n",
        "        self.action_size = N_actions\n",
        "        self.state_size = len(N_space) # (15, 15)\n",
        "        \n",
        "        self.fc1 = nn.Linear(self.state_size, 512)\n",
        "        # self.fc1 = nn.Linear(self.state_size, 512, bias=False)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, self.action_size)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=512)\n",
        "        # self.bn2 = nn.BatchNorm1d(num_features=256)\n",
        "\n",
        "        # self.fc3.bias.data.copy_(torch.from_numpy(tuned_q)) # update bias giving reward_sum values\n",
        "\n",
        "        print(f\"\\n[ DQN pyTorch's {agent_name.title()} AGENT SETUP]\")\n",
        "\n",
        "    def forward(self, input):               \n",
        "        x = self.fc1(input)\n",
        "        # x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        # x = F.gelu(x)\n",
        "        # x = F.silu(x)\n",
        "        x = self.fc2(x)\n",
        "        # x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        # x = F.gelu(x)\n",
        "        # x = F.silu(x) \n",
        "        return self.fc3(x)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # Exploration VS Exploitation\n",
        "        if random.random() > self.epsilon:\n",
        "            self.eval()  \n",
        "            with torch.no_grad():                \n",
        "                state = Variable(torch.from_numpy(state.astype(np.float32)).unsqueeze(0))\n",
        "                return self.forward(state).argmax().item()\n",
        "        return random.randrange(self.action_size) \n",
        "    \n",
        "    def update_exp_epsilon(self, step):\n",
        "        '''Exponential update of the e-greedy rate'''\n",
        "        # decrease the exploration\n",
        "        # self.epsilon = self.ep_end + (1. - self.ep_end) * np.exp(-self.decay_rate * (step))\n",
        "        # self.epsilon = np.exp(-self.decay_rate * step )\n",
        "        self.epsilon = self.ep_end + (1. - self.ep_end) * \\\n",
        "                            np.exp(-self.decay_rate * step )\n",
        "\n",
        "    def update_lin_epsilon(self, step):\n",
        "        '''Linear update of the e-greedy rate'''\n",
        "        start = 1.\n",
        "        end = 0.01 # minimum epsilon\n",
        "        end_fraction = 0.1 # % of total steps to linearly anneal epsilon\n",
        "\n",
        "        def func(progress_remaining: float) -> float:\n",
        "            if (1 - progress_remaining) > end_fraction:\n",
        "                return end\n",
        "            else:\n",
        "                return start + (1 - progress_remaining) * (end - start) / end_fraction\n",
        "        self.epsilon = func(1. -step / STEPS)\n"
      ],
      "metadata": {
        "id": "KDVLFwWxcFOW",
        "outputId": "36c7e608-5ccd-4216-e897-01c5ec027749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN"
      ],
      "metadata": {
        "id": "sabSWW1bcQDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, next_state, reward):        \n",
        "        \"\"\"Save a transition\"\"\"       \n",
        "        self.memory.append(Transition(*(state, action, next_state, reward)))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory.maxlen)\n",
        "\n",
        "def train_step_td_loss_batch(idx, agent):\n",
        "    '''Using the Bellman equation + Memory (Exp) replay'''\n",
        "    agents[agent].train() # set to train due to the batchNorm\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # transposing the batch\n",
        "    batch = Transition(*zip(*transitions))\n",
        "        \n",
        "    states_batch = Variable( torch.from_numpy(np.array(batch.state, dtype=np.float32))  )\n",
        "    next_states_batch = Variable( torch.from_numpy(np.array(batch.next_state, dtype=np.float32))  )\n",
        "    action_batch = Variable(torch.from_numpy(np.array( batch.action)).unsqueeze(1)) \n",
        "    reward_batch = Variable(torch.from_numpy(np.array( batch.reward)))\n",
        "\n",
        "    # # computing Q(s_t, a)\n",
        "    # # pick the q-val from the given action with gather giving the indexes\n",
        "    state_action_values = agents[agent](states_batch).gather(dim=1, index=action_batch[:, :, idx])\n",
        "       \n",
        "    with torch.no_grad():\n",
        "        # compute V(s_t+1) for each next_state\n",
        "        # next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "        # # take the max q-value for action in next_state\n",
        "        # amax() -> max_values\n",
        "        next_state_values = target_agents[agent](next_states_batch).amax(dim=1)\n",
        "\n",
        "        # Compute the expected Q values\n",
        "        expected_state_action_values = reward_batch[:, idx] + (DRATE * next_state_values )\n",
        "\n",
        "    optimizers[agent].zero_grad()  # setting optimizer to zeroes\n",
        "    # Compute loss\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(dim=1))\n",
        "    loss.backward()\n",
        "    # clipping the values inside the model\n",
        "    # for param in agents[agent].parameters(): # clipping\n",
        "    #     param.grad.data.clamp_(-MAX_NORM, MAX_NORM)\n",
        "    # torch.nn.utils.clip_grad_value_(agents[agent].parameters(), 100)\n",
        "    # torch.nn.utils.clip_grad_value_(agents[agent].parameters(), MAX_NORM)\n",
        "    # perform one-step to optim the model\n",
        "    optimizers[agent].step()\n",
        "\n",
        "    return loss.item(), state_action_values.mean().item()"
      ],
      "metadata": {
        "id": "z-mQPgFAcRNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hard"
      ],
      "metadata": {
        "id": "MoksF40ncUDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Huber loss\n",
        "criterion = nn.SmoothL1Loss()\n",
        "TARGET_UPDATE = 5\n",
        "AGENTS = ['player_1', 'player_2']\n",
        "LR = 0.001\n",
        "\n",
        "# for ii, SEED in enumerate([412, 311, 741, 337, 396], 1):\n",
        "for ii, SEED in enumerate([412], 0):\n",
        "\n",
        "    # for LR in [.1, .01, .001, .0001]:\n",
        "    print(f\"\\n\\n\\tSession: {ii}, seed: {SEED}, LR: {LR}\\n\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    memory = ReplayMemory(capacity=100000)\n",
        "\n",
        "    agents = {}\n",
        "    target_agents = {}\n",
        "    action = {}\n",
        "    optimizers = {}\n",
        "    loss_hist = {}\n",
        "    rewards = {}\n",
        "    q_val_hist = {}\n",
        "\n",
        "\n",
        "    # setting agents\n",
        "    for agent in AGENTS: # state(actions p1, actions p2), possible_actions to take\n",
        "    \n",
        "        # double q-learning\n",
        "        agents[agent] = DQN_agent(N_space=(N_actions, N_actions), N_actions=N_actions, tuned_q=reward_sum, agent_name=agent).to(device)\n",
        "        # optimizers[agent] = optim.AdamW(agents[agent].parameters(), lr=LR, amsgrad=True) \n",
        "        optimizers[agent] = optim.Adam(agents[agent].parameters(), lr=LR) \n",
        "        # optimizers[agent] = optim.RMSprop(agents[agent].parameters(), lr=LR) \n",
        "        # optimizers[agent] = optim.SGD(agents[agent].parameters(), lr=LR, momentum=0.9)\n",
        "\n",
        "        ## creating target networks\n",
        "        target_agents[agent] = DQN_agent(N_space=(N_actions, N_actions), N_actions=N_actions, tuned_q=reward_sum, agent_name=f\"target_{agent}\").to(device)\n",
        "        target_agents[agent].load_state_dict( agents[agent].state_dict() ) # copying values from learning model\n",
        "        # Q_target parameters are frozen.\n",
        "        for p in target_agents[agent].parameters():\n",
        "            p.requires_grad = False\n",
        "        target_agents[agent].eval()\n",
        "\n",
        "        action[agent] = -1\n",
        "        loss_hist[agent] = []\n",
        "        rewards[agent] = []\n",
        "        q_val_hist[agent] = []\n",
        "\n",
        "    config = {    \n",
        "            # 'learning rate': 'polynomial', #0.15,\n",
        "            'discount rate': 0.95,\n",
        "            'exploration type': 'exponential epsilon',\n",
        "            'decay rate': 0.00001,\n",
        "            'seed': SEED,\n",
        "\n",
        "            'q-values tuned': False,\n",
        "            'exp replay': len(memory),            \n",
        "            'batch sz': BATCH_SIZE,\n",
        "            'target upd': TARGET_UPDATE,\n",
        "            'batchNorm': False,\n",
        "            'clipping' : False,\n",
        "            'optim': type(optimizers[agent]).__name__,\n",
        "            # 'job_type' : LR\n",
        "        }\n",
        "        \n",
        "    NAME = f\"hard{ii}\"\n",
        "    run = wandb.init(project='Stability', name=NAME, notes=\"DQN with expReplay -- hard upd\"\n",
        "                    , group=type(optimizers[agent]).__name__\n",
        "                    , job_type = f\"{LR}\"\n",
        "                    , tags=['relu', f'{type(optimizers[agent]).__name__} {LR}', 'DQN', 'exp replay']\n",
        "                    , config=config, reinit=True\n",
        "                    )\n",
        "\n",
        "    wandb.watch(agents['player_1'], log='gradients')\n",
        "    wandb.watch(agents['player_2'], log='gradients')\n",
        "\n",
        "    # TRAINING LOOP\n",
        "    # seeds\n",
        "    np.random.seed(SEED)\n",
        "    random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "\n",
        "    state_hist = []\n",
        "    step = 0\n",
        "\n",
        "    start_training = time.time()\n",
        "    for epi in range(4000):\n",
        "    # for epi in range(12):\n",
        "\n",
        "        state = np.random.randint(0, N_actions, size=N_agents)\n",
        "        for _ in range(500):\n",
        "        # for _ in range(1000):\n",
        "\n",
        "            action = np.zeros(N_agents, dtype=int)\n",
        "            for idx, agent in enumerate(AGENTS):       \n",
        "                action[idx] = agents[agent].select_action(state)\n",
        "                        \n",
        "            # play both actions on the given state and get rewards for p1&p2\n",
        "            reward = replay_classic_reward(action)\n",
        "            next_state = action\n",
        "            \n",
        "            # Store the transition in memory\n",
        "            memory.push(state, action, next_state, reward)\n",
        "\n",
        "            if len(memory) > BATCH_SIZE:\n",
        "            \n",
        "                for idx, agent in enumerate(AGENTS):\n",
        "                    # update agent\n",
        "                    loss, q_val = train_step_td_loss_batch(idx, agent)\n",
        "\n",
        "                    # keep track of values\n",
        "                    # loss_hist[agent].append(loss) # loss\n",
        "                    # rewards[agent].append(reward[idx]) # reward\n",
        "                    # q_val_hist[agent].append(q_val) # q value\n",
        "                    \n",
        "                    # update epsilon\n",
        "                    agents[agent].update_exp_epsilon( step ) # exponential\n",
        "                    # agents[agent].update_lin_epsilon(step ) # linear\n",
        "\n",
        "                    # # loggin wandb every n steps\n",
        "                    if step % 25 == 0:\n",
        "                        wandb.log({\n",
        "                                f'q-value_{agent}': q_val,\n",
        "                                f'loss_{agent}': loss,\n",
        "                                f'reward_{agent}': reward[idx],          \n",
        "                                f'epsilon_{agent}': agents[agent].epsilon,                      \n",
        "                                },\n",
        "                                step= step #\n",
        "                            )\n",
        "                step +=1 # updating steps counter\n",
        "            state = next_state\n",
        "\n",
        "        # Update the target network, copying all weights and biases to target DQN\n",
        "        if epi % TARGET_UPDATE == 0:\n",
        "            for agent in AGENTS:\n",
        "                target_agents[agent].load_state_dict(agents[agent].state_dict())\n",
        "    \n",
        "    def test(agents):\n",
        "        for agent in AGENTS:\n",
        "            agents[agent].epsilon = -1\n",
        "            agents[agent].eval()\n",
        "        EPISODES = 20\n",
        "        STEPS = 500\n",
        "        rew1, rew2 = [], []    \n",
        "        for ep in range(EPISODES):\n",
        "\n",
        "            state = np.random.randint(0, N_actions, size=N_agents)\n",
        "            for _ in range(STEPS):\n",
        "                action = np.zeros(N_agents, dtype=int)\n",
        "                for idx, agent in enumerate(AGENTS):       \n",
        "                    action[idx] = agents[agent].select_action(state) \n",
        "                \n",
        "                reward = replay_classic_reward(action)\n",
        "                next_state = action\n",
        "\n",
        "                # gathering episode rewards\n",
        "                rew1.append(reward[0])\n",
        "                rew2.append(reward[1])\n",
        "                state = next_state\n",
        "\n",
        "        r1 = np.mean(rew1)\n",
        "        r2 = np.mean(rew2)\n",
        "        \n",
        "        return r1 + r2 - abs(r1 - r2)\n",
        "\n",
        "    mean_both_reward = test(agents)\n",
        "    wandb.log({'mean_reward_test': mean_both_reward})\n",
        "        \n",
        "    time_taken = asHHMMSS(time.time() - start_training)\n",
        "    print(f'\\n\\t[TOTAL TRAINING TOOK {time_taken} ] \\n')\n",
        "    # wandb.config.update({'time taken': time_taken})\n",
        "    \n",
        "    wandb.finish() # finishing session"
      ],
      "metadata": {
        "id": "cWUl8UY-cVmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ly99MUeEcRsG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}